<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Numa | Zylia's Blog]]></title>
  <link href="http://zylia.github.io/blog/categories/numa/atom.xml" rel="self"/>
  <link href="http://zylia.github.io/"/>
  <updated>2015-03-19T09:39:23+08:00</updated>
  <id>http://zylia.github.io/</id>
  <author>
    <name><![CDATA[zylia]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[系统调用perf_event_open的使用说明]]></title>
    <link href="http://zylia.github.io/blog/2015/03/18/perf-event-open/"/>
    <updated>2015-03-18T16:19:47+08:00</updated>
    <id>http://zylia.github.io/blog/2015/03/18/perf-event-open</id>
    <content type="html"><![CDATA[<p>perf_event_open是用来设置intel的性能监控。perf_event_open()返回一个文件描述符，该描述符可以在系统调用(read(2), mmap(2), prctl(2)等)中使用。</p>

<p>调用perf_event_open()创建一个文件描述符，用来记录性能信息。每个被测量的事件对应一个文件描述符；它们能组合在一起用来同时测量多个事件。</p>

<!-- more -->


<p>启动和关闭性能测量事件对应下面两种方式：ioctl(2)和prctl(2)。当一个事件失效时，它不再记录数据或者产生溢出，但是它仍然存在于内存中并且保持它的记录值。</p>

<p>事件以两种方式被记录：couting和sampled。一个counting事件用来记录事件发生的总的次数。通常情况，counting事件用read(2)系统调用来收集。一个sanmpling事件周期性地写测量数据到一个缓存中，该缓存一般由mmap(2)调用访问。</p>

<h2>概括</h2>

<pre><code>#include &lt;linux/perf_event.h&gt;
#include &lt;linux/hw_breakpoint.h&gt;

int perf_event_open(struct perf_event_attr *attr,
                    pid_t pid, int cpu, int group_fd,
                    unsigned long flags);
</code></pre>

<h2>参数</h2>

<p><em>pid</em>和<em>cpu</em>参数用来描述监控哪个进程和CPU:</p>

<p><strong>pid = 0 和 cpu = -1</strong>  <br />
    用来监控位于任何CPU上的调用进程/线程。</p>

<p><strong>pid = 0 和 cpu >= 0</strong>  <br />
    用来监控位于特定CPU上的调用进程/线程。</p>

<p><strong>pid > 0 和 cpu = -1</strong>  <br />
    用来监控位于任何CPU上的特定进程/线程。</p>

<p><strong>pid > 0 和 cpu >= 0</strong>  <br />
    用来监控位于特定CPU上的特定进程/线程。</p>

<p><strong>pid = -1 和 cpu >= 0</strong>  <br />
用来监控在特定CPU上的所有进程/线程。这需要 <strong>CAP_SYS_ADMIN</strong>特性或者 /proc/sys/kernel/perf_event_paranoid值小于1。</p>

<p><strong>pid = -1 和 cpu = -1</strong>  <br />
这个设置是无效的并会返回错误。<em>group_fd</em>参数允许建立事件组。一个事件组有一个事件被称为组长。首先建立组长，他的<em>group_id</em> = -1。接下来剩下的组员会用组长的文件描述符作为<em>group_fd</em>并用<strong>perf_event_open()</strong>建立。(单一事件将会以<em>group_fd</em> = -1来设定，并且认为该组只有一个成员)。一个事件组做为一个整体调度到CPU上：只有所有组内的事件都能被放在同一CPU上它才会放置在CPU上。这以为着组员事件的数值互相比较是有意义的&mdash;相加，相除，等等&mdash;因为他们是执行相同指令时的事件统计值。</p>

<p><em>flags</em>参数由ORing和以下数值的0个或者多个组成：</p>

<p><strong>PERF_FLAG_FD_CLOEXEC</strong> (since Linux 3.14)</p>

<p>这个标志能使创建的事件文件描述符的close-on-exec标志有效，因此当调用execve(2)时，文件描述符能自动地关闭。应该在文件描述符创建的开始设置close-on-exec标志，而不是后来再用fcntl(2)设置，这能避免潜在的竞争条件，当线程调用perf_event_open()和fcntl(2)发生在和另一个线程调用fork(2)然后执行execve(2)同一时刻的情况。</p>

<p><strong>PERF_FLAG_FD_NO_GROUP</strong></p>

<p>这个标志告诉事件忽略掉<em>group_fd</em>参数，除非是出现使用<strong>PERF_FLAG_FD_OUTPUT</strong>标志重定向输出的情况。</p>

<p><strong>PERF_FLAG_FD_OUTPUT</strong> (broken since Linux 2.6.35)</p>

<p>这个标志重新定位事件的采样输出，而不是按照惯例输出到由<em>group_fd</em>指定事件的mmap缓存中。</p>

<p><strong>PERF_FLAG_PID_CGROUP</strong> (since Linux 2.6.39)</p>

<p>这个标志激活了每个容器的系统范围的监控。一个容器是一系列为了更细化控制而隔离出来的资源的抽象(CPUs, memory等等)。在这个模式下，只有当线程运行在属于被指定的容器中的CPU上(cgroup)。</p>

<p><em>perf_event_attr</em>结构体为创建的事件提供了详细的配置信息。</p>

<pre><code>struct perf_event_attr {
               __u32 type;         /* Type of event */
               __u32 size;         /* Size of attribute structure */
               __u64 config;       /* Type-specific configuration */

               union {
                   __u64 sample_period;    /* Period of sampling */
                   __u64 sample_freq;      /* Frequency of sampling */
               };

               __u64 sample_type;  /* Specifies values included in sample */
               __u64 read_format;  /* Specifies values returned in read */

               __u64 disabled       : 1,   /* off by default */
                     inherit        : 1,   /* children inherit it */
                     pinned         : 1,   /* must always be on PMU */
                     exclusive      : 1,   /* only group on PMU */
                     exclude_user   : 1,   /* don't count user */
                     exclude_kernel : 1,   /* don't count kernel */
                     exclude_hv     : 1,   /* don't count hypervisor */
                     exclude_idle   : 1,   /* don't count when idle */
                     mmap           : 1,   /* include mmap data */
                     comm           : 1,   /* include comm data */
                     freq           : 1,   /* use freq, not period */
                     inherit_stat   : 1,   /* per task counts */
                     enable_on_exec : 1,   /* next exec enables */
                     task           : 1,   /* trace fork/exit */
                     watermark      : 1,   /* wakeup_watermark */
                     precise_ip     : 2,   /* skid constraint */
                     mmap_data      : 1,   /* non-exec mmap data */
                     sample_id_all  : 1,   /* sample_type all events */
                     exclude_host   : 1,   /* don't count in host */
                     exclude_guest  : 1,   /* don't count in guest */
                     exclude_callchain_kernel : 1,
                                           /* exclude kernel callchains */
                     exclude_callchain_user   : 1,
                                           /* exclude user callchains */
                     mmap2          :  1,  /* include mmap with inode data */
                     comm_exec      :  1,  /* flag comm events that are due to exec */
                     __reserved_1   : 39;

               union {
                   __u32 wakeup_events;    /* wakeup every n events */
                   __u32 wakeup_watermark; /* bytes before wakeup */
               };

               __u32     bp_type;          /* breakpoint type */

               union {
                   __u64 bp_addr;          /* breakpoint address */
                   __u64 config1;          /* extension of config */
               };

               union {
                   __u64 bp_len;           /* breakpoint length */
                   __u64 config2;          /* extension of config1 */
               };
               __u64 branch_sample_type;   /* enum perf_branch_sample_type */
               __u64 sample_regs_user;     /* user regs to dump on samples */
               __u32 sample_stack_user;    /* size of stack to dump on
                                              samples */
               __u32 __reserved_2;         /* Align to u64 */

           };
</code></pre>

<p><em>perf_event_attr</em>中的各个域描述如下：</p>

<p><em>type</em> 描述了整个事件类型。它的值是以下几种类型之一：</p>

<p>-<strong>PERF_TYPE_HARDWARE</strong></p>

<p>-<strong>PERF_TYPE_SOFTWARE</strong></p>

<p>-<strong>PERF_TYPE_TRACEPOINT</strong></p>

<p>-<strong>PERF_TYPE_HW_CACHE</strong></p>

<p>-<strong>PERF_TYPE_RAW</strong></p>

<p>-<strong>PERF_TYPE_BREAKPOINT</strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NUMA架构的程序性能分析工具MemProf]]></title>
    <link href="http://zylia.github.io/blog/2015/03/18/memprof/"/>
    <updated>2015-03-18T15:40:50+08:00</updated>
    <id>http://zylia.github.io/blog/2015/03/18/memprof</id>
    <content type="html"><![CDATA[<p>&ldquo;MemProf: a Memory Profiler for NUMA Multicore Systems&#8221;是发表在计算机系统领域顶级会议USENIX Annual Technical Conference(ATC &lsquo;14)上。作者提出了一个分析工具，能够建立线程和内存对象之间的交互信息，帮助程序员理解为什么和哪些内存对象是远端访问。</p>

<!-- more -->


<p>应用程序级别的优化技术有以下一些缺点；对于程序员来说很难决定一个给定的程序/工作集可以用哪些优化策略。我们需要知道线程和内存对象之间的交互信息，例如，在程序运行的任何时刻点知道哪些线程访问哪些内存对象，以及每次内存访问请求的发送和接受节点的详细信息。然而，现有的分析工具，例如OProfile，Linux Perf，VTune和Memphis都无法提供这些信息。这些工具有些能够提供全局静态内存对象的信息，但是这些对象只占据了所有远端内存访问的很小一部分比例。作者做实验发现这些全局静态内存对象只占用了少于4%的总远端访问次数。对于其他内存对象，现有的分析工具只提供了目标内存地址和触发该访问的程序指令。</p>
]]></content>
  </entry>
  
</feed>
