<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Numa | Zylia's Blog]]></title>
  <link href="http://zylia.github.io/blog/categories/numa/atom.xml" rel="self"/>
  <link href="http://zylia.github.io/"/>
  <updated>2015-03-18T20:03:55+08:00</updated>
  <id>http://zylia.github.io/</id>
  <author>
    <name><![CDATA[zylia]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[系统调用perf_event_open的使用说明]]></title>
    <link href="http://zylia.github.io/blog/2015/03/18/perf-event-open/"/>
    <updated>2015-03-18T16:19:47+08:00</updated>
    <id>http://zylia.github.io/blog/2015/03/18/perf-event-open</id>
    <content type="html"><![CDATA[<p>perf_event_open是用来设置intel的性能监控。perf_event_open()返回一个文件描述符，该描述符可以在系统调用(read(2), mmap(2), prctl(2)等)中使用。</p>

<p>调用perf_event_open()创建一个文件描述符，用来记录性能信息。每个被测量的事件对应一个文件描述符；它们能组合在一起用来同时测量多个事件。</p>

<!-- more -->


<p>启动和关闭性能测量事件对应下面两种方式：ioctl(2)和prctl(2)。当一个事件失效时，它不再记录数据或者产生溢出，但是它仍然存在于内存中并且保持它的记录值。</p>

<p>事件以两种方式被记录：couting和sampled。一个counting事件用来记录事件发生的总的次数。通常情况，counting事件用read(2)系统调用来收集。一个sanmpling事件周期性地写测量数据到一个缓存中，该缓存一般由mmap(2)调用访问。</p>

<h2>概括</h2>

<pre><code>#include &lt;linux/perf_event.h&gt;
#include &lt;linux/hw_breakpoint.h&gt;

int perf_event_open(struct perf_event_attr *attr,
                    pid_t pid, int cpu, int group_fd,
                    unsigned long flags);
</code></pre>

<h2>参数</h2>

<p><em>pid</em>和<em>cpu</em>参数用来描述监控哪个进程和CPU:</p>

<p><strong>pid = 0 和 cpu = -1</strong>  <br />
    用来监控位于任何CPU上的调用进程/线程。</p>

<p><strong>pid = 0 和 cpu >= 0</strong>  <br />
    用来监控位于特定CPU上的调用进程/线程。</p>

<p><strong>pid > 0 和 cpu = -1</strong>  <br />
    用来监控位于任何CPU上的特定进程/线程。</p>

<p><strong>pid > 0 和 cpu >= 0</strong>  <br />
    用来监控位于特定CPU上的特定进程/线程。</p>

<p><strong>pid = -1 和 cpu >= 0</strong>  <br />
用来监控在特定CPU上的所有进程/线程。这需要 <strong>CAP_SYS_ADMIN</strong>特性或者 /proc/sys/kernel/perf_event_paranoid值小于1。</p>

<p><strong>pid = -1 和 cpu = -1</strong>  <br />
这个设置是无效的并会返回错误。<em>group_fd</em>参数允许建立事件组。一个事件组有一个事件被称为组长。首先建立组长，他的<em>group_id</em> = -1。接下来剩下的组员会用组长的文件描述符作为<em>group_fd</em>并用<strong>perf_event_open()</strong>建立。(单一事件将会以<em>group_fd</em> = -1来设定，并且认为该组只有一个成员)。一个事件组做为一个整体调度到CPU上：只有所有组内的事件都能被放在同一CPU上它才会放置在CPU上。这以为着组员事件的数值互相比较是有意义的&mdash;相加，相除，等等&mdash;因为他们是执行相同指令时的事件统计值。</p>

<p><em>flags</em>参数由ORing和以下数值的0个或者多个组成：</p>

<p><strong>PERF_FLAG_FD_CLOEXEC</strong> (since Linux 3.14)</p>

<p>这个标志能使创建的事件文件描述符的close-on-exec标志有效，因此当调用execve(2)时，文件描述符能自动地关闭。应该在文件描述符创建的开始设置close-on-exec标志，而不是后来再用fcntl(2)设置，这能避免潜在的竞争条件，当线程调用perf_event_open()和fcntl(2)发生在和另一个线程调用fork(2)然后执行execve(2)同一时刻的情况。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NUMA架构的程序性能分析工具MemProf]]></title>
    <link href="http://zylia.github.io/blog/2015/03/18/memprof/"/>
    <updated>2015-03-18T15:40:50+08:00</updated>
    <id>http://zylia.github.io/blog/2015/03/18/memprof</id>
    <content type="html"><![CDATA[<p>&ldquo;MemProf: a Memory Profiler for NUMA Multicore Systems&#8221;是发表在计算机系统领域顶级会议USENIX Annual Technical Conference(ATC &lsquo;14)上。作者提出了一个分析工具，能够建立线程和内存对象之间的交互信息，帮助程序员理解为什么和哪些内存对象是远端访问。</p>

<!-- more -->


<p>应用程序级别的优化技术有以下一些缺点；对于程序员来说很难决定一个给定的程序/工作集可以用哪些优化策略。我们需要知道线程和内存对象之间的交互信息，例如，在程序运行的任何时刻点知道哪些线程访问哪些内存对象，以及每次内存访问请求的发送和接受节点的详细信息。然而，现有的分析工具，例如OProfile，Linux Perf，VTune和Memphis都无法提供这些信息。这些工具有些能够提供全局静态内存对象的信息，但是这些对象只占据了所有远端内存访问的很小一部分比例。作者做实验发现这些全局静态内存对象只占用了少于4%的总远端访问次数。对于其他内存对象，现有的分析工具只提供了目标内存地址和触发该访问的程序指令。</p>
]]></content>
  </entry>
  
</feed>
