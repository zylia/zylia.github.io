<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Numa | Zylia's Blog]]></title>
  <link href="http://zylia.github.io/blog/categories/numa/atom.xml" rel="self"/>
  <link href="http://zylia.github.io/"/>
  <updated>2015-03-23T16:24:36+08:00</updated>
  <id>http://zylia.github.io/</id>
  <author>
    <name><![CDATA[zylia]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[收集多线程程序的性能数据]]></title>
    <link href="http://zylia.github.io/blog/2015/03/20/perf-event-open-lab/"/>
    <updated>2015-03-20T11:07:56+08:00</updated>
    <id>http://zylia.github.io/blog/2015/03/20/perf-event-open-lab</id>
    <content type="html"><![CDATA[<p>该篇blog讲如何用perf_event_open()系统调用收集多线程程序的性能数据。</p>

<p>首先，我们需要检测该多线程程序开始运行后分配的进程号，以便后来能够跟踪。我们会用vfork()创建一个子进程，子进程和父进程共享同样的代码逻辑，并且都是从vfork()语句之后执行。子进程会先执行，父进程会先被阻塞。如果子进程调用exit()或者exec()，那么父进程会解除阻塞状态，恢复执行。<!-- more -->然后在子进程中调用execvp()运行多线程程序，一旦子进程调用execvp()，子进程就会返回，父进程会继续运行，这样我们就能获得多线程程序在运行时分配的进程号。vfork()系统调用会分别在父进程和子进程中返回。父进程会返回子进程的进程号，而在子进程中会返回0。</p>

<pre><code>sprintf(line,"/parsec-2.1/pkgs/apps/blackscholes/inst/amd64-linux.gcc-openmp/bin/blackscholes 8 /parsec-2.1/pkgs/apps/blackscholes/inputs/in_64K.txt output.txt");
char *cmd[] = {"sh", "-c", line, NULL};

iResult = vfork();
if (iResult &lt; 0)
{
     int err_code = errno;
     printf ("*** ERROR: fork failed for %s\n", strerror(err_code));
}
else if (iResult)  // 对于child process该值为0
{
     return iResult;
}
if (execvp("sh", cmd) &lt; 0){
     int err_code = errno;
     printf ("*** ERROR: exec failed for %s\n",strerror(err_code));
}
</code></pre>

<p>在获得了多线程程序的进程号之后，我们需要知道该程序派生的线程数目以及它们的线程号。程序派生的线程号可以在&#8221;/proc/pid/task/&ldquo;目录中查看，我们通过统计该目录下的文件的个数并把文件名转换为线程号。</p>

<p>在获得了程序所有的线程号后，我们给每一个线程用perf_event_open()建立一个记录性能数据的文件。首先，需要初始化一个perf_event_attr结构体，其中config1参数代表记录的延迟阈值。perf_event_open()新建了一个文件，并返回了该文件的文件描述符。当perf_event_open()工作在采样模式时，我们需要将该文件映射到内存才能进行读写操作。采用下面的语句将文件映射到length大小的缓存中，</p>

<pre><code>metadata_page[index]= mmap(NULL, length, PROT_READ, MAP_SHARED, curfd, 0);
</code></pre>

<p>在该调用中，我们把perf_event_open()返回的文件描述符curfd作为参数传递给mmap()调用。然后，性能数据就会记录到metadata_page当中。</p>

<p>接着调用PERF_EVENT_IOC_RESET和PERF_EVENT_IOC_ENABLE，性能监控事件开始运行。</p>

<p>当多线程程序运行结束后，调用PERF_EVENT_IOC_DISABLE，停止记录性能监控数据。过滤掉metadata_page的头文件后，剩下的就是记录的性能采样数据。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[系统调用perf_event_open的使用说明]]></title>
    <link href="http://zylia.github.io/blog/2015/03/18/perf-event-open/"/>
    <updated>2015-03-18T16:19:47+08:00</updated>
    <id>http://zylia.github.io/blog/2015/03/18/perf-event-open</id>
    <content type="html"><![CDATA[<p>perf_event_open是用来实现intel的性能监控的功能。perf_event_open()返回一个文件描述符，该描述符可以在系统调用(read(2), mmap(2), prctl(2)等)中使用。</p>

<p>调用perf_event_open()创建一个文件描述符，用来记录性能信息。每个被测量的事件对应一个文件描述符；它们能组合在一起同时测量多个事件。</p>

<!-- more -->


<p>启动和关闭性能测量事件对应下面两种方式：ioctl(2)和prctl(2)。当一个事件被失效时，它不再记录数据或者产生溢出，但是它仍然存在于内存中并且保持它的记录值。</p>

<p>事件以两种方式被记录：couting和sampled。一个counting事件用来记录事件发生的总的次数。通常情况，counting事件用read(2)系统调用来收集。一个sampling事件周期性地写测量数据到一个缓存中，该缓存一般由mmap(2)调用访问。</p>

<h2>概括</h2>

<pre><code>#include &lt;linux/perf_event.h&gt;
#include &lt;linux/hw_breakpoint.h&gt;

int perf_event_open(struct perf_event_attr *attr,
                    pid_t pid, int cpu, int group_fd,
                    unsigned long flags);
</code></pre>

<h2>pid和cpu参数</h2>

<p><em>pid</em>和<em>cpu</em>参数用来描述监控哪个进程和CPU:</p>

<p><strong>pid = 0 和 cpu = -1</strong>  <br />
    用来监控位于任何CPU上的调用进程/线程。</p>

<p><strong>pid = 0 和 cpu >= 0</strong>  <br />
    用来监控位于特定CPU上的调用进程/线程。</p>

<p><strong>pid > 0 和 cpu = -1</strong>  <br />
    用来监控位于任何CPU上的特定进程/线程。</p>

<p><strong>pid > 0 和 cpu >= 0</strong>  <br />
    用来监控位于特定CPU上的特定进程/线程。</p>

<p><strong>pid = -1 和 cpu >= 0</strong>  <br />
用来监控在特定CPU上的所有进程/线程。这需要 <strong>CAP_SYS_ADMIN</strong>特性或者 /proc/sys/kernel/perf_event_paranoid值小于1。</p>

<p><strong>pid = -1 和 cpu = -1</strong>  <br />
这个设置是无效的并会返回错误。</p>

<h2>group_fd参数</h2>

<p><em>group_fd</em>参数允许建立事件组。一个事件组有一个事件被称为组长。首先建立组长，他的<em>group_id</em> = -1。接下来剩下的组员会用组长的文件描述符作为<em>group_fd</em>并用<strong>perf_event_open()</strong>建立。(单一事件将会以<em>group_fd</em> = -1来设定，并且认为该组只有一个成员)。一个事件组做为一个整体调度到CPU上：只有所有组内的事件都能被放在同一CPU上它才会放置在CPU上。这以为着组员事件的数值直接操作是有意义的&mdash;相加，相除，等等&mdash;因为他们是执行相同指令过程中的事件统计值。</p>

<h2>flag参数</h2>

<p><em>flags</em>参数由ORing和以下数值的0个或者多个组成：</p>

<p><strong>PERF_FLAG_FD_CLOEXEC</strong> (since Linux 3.14)</p>

<p>这个标志能使创建的事件文件描述符的close-on-exec标志有效，因此当调用execve(2)时，文件描述符能自动地关闭。应该在文件描述符创建的开始设置close-on-exec标志，而不是后来再用fcntl(2)设置，这能避免潜在的竞争条件，当线程调用perf_event_open()和fcntl(2)发生在另一个线程调用fork(2)然后执行execve(2)同一时刻的情况。</p>

<p><strong>PERF_FLAG_FD_NO_GROUP</strong></p>

<p>这个标志告诉事件忽略掉<em>group_fd</em>参数，除非是出现使用<strong>PERF_FLAG_FD_OUTPUT</strong>标志重定向输出的情况。</p>

<p><strong>PERF_FLAG_FD_OUTPUT</strong> (broken since Linux 2.6.35)</p>

<p>这个标志重新定位事件的采样输出，而不是按照惯例输出到由<em>group_fd</em>指定事件的mmap缓存中。</p>

<p><strong>PERF_FLAG_PID_CGROUP</strong> (since Linux 2.6.39)</p>

<p>这个标志激活了系统中每个容器范围内的监控。一个容器是一系列为了更细化控制而隔离出来的资源的抽象(CPUs, memory等等)。在这个模式下，只有当线程运行在属于被指定的容器中的CPU上(cgroup)时才进行监控。</p>

<h2>perf_event_attr参数</h2>

<p><em>perf_event_attr</em>结构体为创建的事件提供了详细的配置信息。</p>

<pre><code>struct perf_event_attr {
     __u32 type;         /* Type of event */
     __u32 size;         /* Size of attribute structure */
     __u64 config;       /* Type-specific configuration */

     union {
         __u64 sample_period;    /* Period of sampling */
         __u64 sample_freq;      /* Frequency of sampling */
     };

     __u64 sample_type;  /* Specifies values included in sample */
     __u64 read_format;  /* Specifies values returned in read */

     __u64 disabled       : 1,   /* off by default */
           inherit        : 1,   /* children inherit it */
           pinned         : 1,   /* must always be on PMU */
           exclusive      : 1,   /* only group on PMU */
           exclude_user   : 1,   /* don't count user */
           exclude_kernel : 1,   /* don't count kernel */
           exclude_hv     : 1,   /* don't count hypervisor */
           exclude_idle   : 1,   /* don't count when idle */
           mmap           : 1,   /* include mmap data */
           comm           : 1,   /* include comm data */
           freq           : 1,   /* use freq, not period */
           inherit_stat   : 1,   /* per task counts */
           enable_on_exec : 1,   /* next exec enables */
           task           : 1,   /* trace fork/exit */
           watermark      : 1,   /* wakeup_watermark */
           precise_ip     : 2,   /* skid constraint */
           mmap_data      : 1,   /* non-exec mmap data */
           sample_id_all  : 1,   /* sample_type all events */
           exclude_host   : 1,   /* don't count in host */
           exclude_guest  : 1,   /* don't count in guest */
           exclude_callchain_kernel : 1,
                           /* exclude kernel callchains */
           exclude_callchain_user   : 1,
                           /* exclude user callchains */
           mmap2          :  1,  /* include mmap with inode data */
           comm_exec      :  1,  /* flag comm events that are due to exec */
           __reserved_1   : 39;

     union {
         __u32 wakeup_events;    /* wakeup every n events */
         __u32 wakeup_watermark; /* bytes before wakeup */
     };

     __u32     bp_type;          /* breakpoint type */

     union {
         __u64 bp_addr;          /* breakpoint address */
         __u64 config1;          /* extension of config */
     };

     union {
         __u64 bp_len;           /* breakpoint length */
         __u64 config2;          /* extension of config1 */
     };
     __u64 branch_sample_type;   /* enum perf_branch_sample_type */
     __u64 sample_regs_user;     /* user regs to dump on samples */
     __u32 sample_stack_user;    /* size of stack to dump on
                                    samples */
     __u32 __reserved_2;         /* Align to u64 */

};
</code></pre>

<p><em>perf_event_attr</em>中的各个域多并且复杂，我们分别叙述如下：</p>

<p>1)<em>type</em>域设置</p>

<p><em>type</em> 描述了整个事件类型。它的值是以下几种类型之一：</p>

<p><font size=3>
　　<strong>PERF_TYPE_HARDWARE</strong>  <br />
　　<strong>PERF_TYPE_SOFTWARE</strong>  <br />
　　<strong>PERF_TYPE_TRACEPOINT</strong>  <br />
　　<strong>PERF_TYPE_HW_CACHE</strong>  <br />
　　<strong>PERF_TYPE_RAW</strong>  <br />
　　<strong>PERF_TYPE_BREAKPOINT</strong></font></p>

<p>2)<em>size</em>域设置</p>

<p><em>size</em> 描述了<em>perf_event_attr</em>结构体的大小。</p>

<p>3)<em>config</em>域设置</p>

<p><em>config</em> 和<em>type</em>域一起使用，描述你想要的事件类型。当64位无法满足描述事件类型的要求，<em>config1</em>和<em>config2</em>也被拿来一起使用。这些域的编码依赖于特定的事件。</p>

<p><em>config</em>域的类型有很多种，该值依赖于前面描述的<em>type</em>域。随着<em>type</em>域的不同，设置<em>config</em>域的类型也不尽相同。</p>

<p>如果<em>type</em>是<strong>PERF_TYPE_HARDWARE</strong>，我们在测量普通的CPU硬件事件。并不是这些事件都适合所有的平台。设置<em>config</em>为下面值的其中之一：</p>

<p><font size=3>
　　<strong>PERF_COUNT_HW_CPU_CYCLES</strong>  <br />
　　<strong>PERF_COUNT_HW_INSTRUCTIONS</strong>  <br />
　　<strong>PERF_COUNT_HW_CACHE_REFERENCES</strong>  <br />
　　<strong>PERF_COUNT_HW_CACHE_MISSES</strong>  <br />
　　<strong>PERF_COUNT_HW_BRANCH_INSTRUCTIONS</strong>  <br />
　　<strong>PERF_COUNT_HW_BRANCH_MISSES</strong>  <br />
　　<strong>PERF_COUNT_HW_BUS_CYCLES</strong>  <br />
　　<strong>PERF_COUNT_HW_STALLED_CYCLES_FRONTEND</strong>：在发送指令时的延迟。  <br />
　　<strong>PERF_COUNT_HW_STALLED_CYCLES_BACKEND</strong>：指令执行完后的延迟。  <br />
　　<strong>PERF_COUNT_HW_REF_CPU_CYCLES</strong></font></p>

<p>如果<em>type</em>是<strong>PERF_TYPE_SOFTWARE</strong>，我们在测量内核提供的软件事件。设置<em>config</em>为以下几种类型之一：</p>

<p><font size=3>
　　<strong>PERF_COUNT_SW_CPU_CLOCK</strong>  <br />
　　<strong>PERF_COUNT_SW_TASK_CLOCK</strong>  <br />
　　<strong>PERF_COUNT_SW_PAGE_FAULTS</strong>  <br />
　　<strong>PERF_COUNT_SW_CONTEXT_SWITCHES</strong>  <br />
　　<strong>PERF_COUNT_SW_CPU_MIGRATIONS</strong>  <br />
　　<strong>PERF_COUNT_SW_PAGE_FAULTS_MIN</strong>  <br />
　　<strong>PERF_COUNT_SW_PAGE_FAULTS_MAJ</strong></font> 等等。</p>

<p>如果<em>type</em>是<strong>PERF_TYPE_RAW</strong>，那么需要一个定制的“raw” config值。大多数CPU支持的事件不包含在普通事件中。它们是由实现定义的；参照你的CPU手册(例如Intel Volume 3B文档)。libpfm4库能够用来转换体系结构手册里的名字到perf_event_open()在这个域中想要的十六进制值。</p>

<p>4)<em>sample_type</em>域设置</p>

<p>用来描述sample中包含哪些值。</p>

<h2>读取结果</h2>

<h3>以计数模式运行的情况</h3>

<p>当<strong>perf_event_open()</strong>文件描述符被打开后，事件的值能够从文件描述符中读取。这个值通过<em>attr</em>结构体的<em>read_format</em>域设定。</p>

<p>如果你想要存储的缓存无法放下所有的数据，将会返回<strong>ENOSPC</strong>。</p>

<h3>以采样模式运行的情况</h3>

<p>当以采样模式使用<strong>perf_event_open()</strong>时，事件的结果记录在一个环形缓存中。该环形缓存用mmap(2)创建和访问。</p>

<p>mmap的大小应该是1+2<sup>n</sup> 页，第一个页是metadata页(<em>struct perf_event_mmap_page</em>)，它包括环形缓存在内存中起始位置的信息。</p>

<p>第一个metadata mmap页的结构如下所示：</p>

<pre><code>struct perf_event_mmap_page {
     __u32 version;        /* version number of this structure */
     __u32 compat_version; /* lowest version this is compat with */
     __u32 lock;           /* seqlock for synchronization */
     __u32 index;          /* hardware counter identifier */
     __s64 offset;         /* add to hardware counter value */
     __u64 time_enabled;   /* time event active */
     __u64 time_running;   /* time event on CPU */
     union {
         __u64   capabilities;
         struct {
             __u64 cap_usr_time / cap_usr_rdpmc / cap_bit0 : 1,
             cap_bit0_is_deprecated : 1,
             cap_user_rdpmc         : 1,
             cap_user_time          : 1,
             cap_user_time_zero     : 1,
         };
     };
     __u16 pmc_width;
     __u16 time_shift;
     __u32 time_mult;：
     __u64 time_offset;
     __u64 __reserved[120];   /* Pad to 1k */
     __u64 data_head;         /* head in the data section */
     __u64 data_tail;         /* user-space written tail */
}
</code></pre>

<p>剩下的2<sup>n</sup>环形缓存页具有如下所示的布局：</p>

<p>该环形缓存的mmap值以一个header开始：</p>

<pre><code>struct perf_event_header {
     __u32   type;
     __u16   misc;
     __u16   size;
};
</code></pre>

<p>如果<em>type</em>的值是<strong>PERF_RECORD_SAMPLE</strong>，那么记录的信息如下面的格式:</p>

<pre><code>struct {
     struct perf_event_header header;
     u64   sample_id;  /* if PERF_SAMPLE_IDENTIFIER */
     u64   ip;         /* if PERF_SAMPLE_IP */
     u32   pid, tid;   /* if PERF_SAMPLE_TID */
     u64   time;       /* if PERF_SAMPLE_TIME */
     u64   addr;       /* if PERF_SAMPLE_ADDR */
     u64   id;         /* if PERF_SAMPLE_ID */
     u64   stream_id;  /* if PERF_SAMPLE_STREAM_ID */
     u32   cpu, res;   /* if PERF_SAMPLE_CPU */
     u64   period;     /* if PERF_SAMPLE_PERIOD */
     struct read_format v; /* if PERF_SAMPLE_READ */
     u64   nr;         /* if PERF_SAMPLE_CALLCHAIN */
     u64   ips[nr];    /* if PERF_SAMPLE_CALLCHAIN */
     u32   size;       /* if PERF_SAMPLE_RAW */
     char  data[size]; /* if PERF_SAMPLE_RAW */
     u64   bnr;        /* if PERF_SAMPLE_BRANCH_STACK */
     struct perf_branch_entry lbr[bnr];
                       /* if PERF_SAMPLE_BRANCH_STACK */
     u64   abi;        /* if PERF_SAMPLE_REGS_USER */
     u64   regs[weight(mask)];
                       /* if PERF_SAMPLE_REGS_USER */
     u64   size;       /* if PERF_SAMPLE_STACK_USER */
     char  data[size]; /* if PERF_SAMPLE_STACK_USER */
     u64   dyn_size;   /* if PERF_SAMPLE_STACK_USER */
     u64   weight;     /* if PERF_SAMPLE_WEIGHT */
     u64   data_src;   /* if PERF_SAMPLE_DATA_SRC */
     u64   transaction;/* if PERF_SAMPLE_TRANSACTION */
};
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NUMA架构的程序性能分析工具MemProf]]></title>
    <link href="http://zylia.github.io/blog/2015/03/18/memprof/"/>
    <updated>2015-03-18T15:40:50+08:00</updated>
    <id>http://zylia.github.io/blog/2015/03/18/memprof</id>
    <content type="html"><![CDATA[<p>&ldquo;MemProf: a Memory Profiler for NUMA Multicore Systems&#8221;是发表在计算机系统领域顶级会议USENIX Annual Technical Conference(ATC &lsquo;14)上。作者提出了一个分析工具，能够建立线程和内存对象之间的交互信息，帮助程序员理解为什么和哪些内存对象是远端访问。</p>

<!-- more -->


<p>作者在文章中指出，应用程序级别的优化技术有以下一些缺点；对于程序员来说很难决定一个给定的程序/工作集可以用哪些优化策略。我们需要知道线程和内存对象之间的交互信息，例如，在程序运行的任何时刻点知道哪些线程访问哪些内存对象，以及每次内存访问请求的发送和接受节点的详细信息。然而，现有的分析工具，例如OProfile，Linux Perf，VTune和Memphis都无法提供这些信息。这些工具有些能够提供全局静态内存对象的信息，但是这些对象只占据了所有远端内存访问的很小一部分比例。作者做实验发现这些全局静态内存对象只占用了少于4%的总远端访问次数。对于其他内存对象，现有的分析工具只提供了有限的信息，例如指令访问的目标内存地址和触发该访问的程序指令。</p>

<p>MemProf建立了线程和它访问的内存对象之间的关联。</p>
]]></content>
  </entry>
  
</feed>
